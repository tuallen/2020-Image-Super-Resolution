{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WChZ7M5OUCiL"
   },
   "source": [
    "# **Single Image Super-Resolution** \n",
    "**Joshua Lo (Peer Mentor), Justin Ashbaugh, Jared Habermehl, Allen Tu, Addison Waller**\n",
    "\n",
    "Super-resolution is the process of recovering a high resolution (HR) image or video from its low resolution (LR) counterpart. It has a myriad of applications in many fields, including autonomous vehicles, medical imaging, security, and entertainment. \n",
    "![diagram](https://www.mathworks.com/help/examples/deeplearning_shared/win64/VeryDeepSuperResolutionDeepLearningExample_01.png)\n",
    "Machine learning super resolution uses a model trained with a dataset of images to predict additional pixels from a LR image input, essentially \"filling in\" the gaps in between the pixels of a LR image to create a HR output. We refer to a recovered HR image as a super-resolved (SR) image. A SR image has more pixels than the LR image that it was created from, so it contains more information and will be appear clearer due to its higher pixel density.\n",
    "\n",
    "This notebook demonstrates our single image super-resolution (SISR) program and upscales a still image to 3 times its original size. Our full project can be found on our team's [GitHub](https://github.com/umd-fire-coml/2020-Image-Super-Resolution) repository. It is written in TensorFlow and uses the following imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AnuzNlsW7N0g"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.python.keras.utils.data_utils import Sequence\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "from sys import maxsize\n",
    "import random\n",
    "import requests\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zBIqIjbY8X_8"
   },
   "source": [
    "## Designing the Model\n",
    "Our [model](https://github.com/umd-fire-coml/2020-Image-Super-Resolution/blob/master/model.md) uses a series of convolutional layers to extract, or learn, information from the LR image. Then, it combines the data that it collected to create the SR image.  \n",
    "\n",
    "![architecture](https://miro.medium.com/max/4902/1*n4cXo7DASn1_HEGrDNJVFg.png)\n",
    "\n",
    "In technical terms, this is a seven-layer [Efficient Sub-Pixel Convolutional Neural Network (ESPCN)](https://arxiv.org/pdf/1609.05158.pdf) SISR model, which takes a LR image input, extracts LR feature maps through a series of convolutional layers, then applies a sub-pixel convolution layer to assemble the LR feature maps into a HR image output. You can learn more about our model and how we arrived at it in its [documentation](https://github.com/umd-fire-coml/2020-Image-Super-Resolution/blob/master/model.md). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NQIFhMAI22dq"
   },
   "outputs": [],
   "source": [
    "# 6-layer ESPCN SISR model\n",
    "def espcn_model(r, channels = 3):\n",
    "    # Arguments for Conv2D\n",
    "    conv_args = {\n",
    "      \"activation\": \"relu\",\n",
    "      \"padding\" : \"same\",\n",
    "    }\n",
    "    # Input\n",
    "    inputs = keras.Input(shape=(None, None, channels))\n",
    "    # Feature Maps Extraction\n",
    "    conv1 = layers.Conv2D(64, 5, **conv_args)(inputs)\n",
    "    conv2 = layers.Conv2D(64, 3, **conv_args)(conv1)\n",
    "    conv3 = layers.Conv2D(32, 3, **conv_args)(conv2)\n",
    "    conv4 = layers.Conv2D(32, 3, **conv_args)(conv3)\n",
    "    conv5 = layers.Conv2D(32, 3, **conv_args)(conv4)\n",
    "    conv6 = layers.Conv2D(channels*(r*r), 3, **conv_args)(conv5)\n",
    "    # Efficient Sub-Pixel Convolutional Layer\n",
    "    outputs = tf.nn.depth_to_space(conv6, r)\n",
    "    return Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wCeHN-RBdsRD"
   },
   "source": [
    "The upscale factor, `r`, represents how much the model will upscale the LR image. For example, `r=3` below, so our SR image will be 3 times taller and wider in pixels than the LR image. It will also look sharper than the LR image if they are displayed at the same size because it has 9 times as many pixels. \n",
    "\n",
    "Training an effective SISR model takes hours, so we [pre-trained our model](https://github.com/umd-fire-coml/2020-Image-Super-Resolution/blob/master/training.ipynb) using a [dataset of 900 images](https://data.vision.ee.ethz.ch/cvl/DIV2K/) over 100 epochs to save you (a lot) of time. We'll load in those saved weights after we mount the drive. For now, we compile the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fDKay-7H2aYx"
   },
   "outputs": [],
   "source": [
    "r = 3 # Upscale Factor \n",
    "\n",
    "# Compile model\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "def PSNR(y_true, y_pred):\n",
    "    max_pixel = 1.0\n",
    "    return tf.image.psnr(y_true, y_pred, max_val=max_pixel)\n",
    "model = espcn_model(r)\n",
    "model.compile(optimizer=opt, loss='mse', metrics=[PSNR])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jBLjcvY08OHK"
   },
   "source": [
    "## Loading and Generating Data\n",
    "Now, we load the aforementioned pre-trained weights into our model. The `testing_dict()` function finds all of the images in our testing dataset and creates a dictionary matching each unique image to its scales. We also have a [`training_dict()`](https://github.com/umd-fire-coml/2020-Image-Super-Resolution/blob/master/dictionary.py) function, but we left it out here because we already trained the model. The training and testing datasets all support `r=2`, `3`, and `4`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NNbUiQBH2PxW"
   },
   "outputs": [],
   "source": [
    "# Load pre-trained weights\n",
    "filepath = \"model/weights/r3bs10epochs100weights.h5\"\n",
    "model.load_weights(filepath)\n",
    "\n",
    "# Returns a dictionary containing all classical SR filepaths\n",
    "def testing_dict():\n",
    "    data_directory = 'data/datasets/'\n",
    "    datasets = ['BSDS100', 'BSDS200', 'General100', 'historical', 'Set5', 'Set14', 'T91', 'urban100', 'manga109']\n",
    "    scales = ['LRbicx2', 'LRbicx3', 'LRbicx4']\n",
    "\n",
    "    # key = image name without directory path\n",
    "    # value = dict of filepaths of original and scaled images\n",
    "    images = {}\n",
    "    # Build images dict for all classical SR images\n",
    "    for dataset in datasets:\n",
    "        dataset_directory = data_directory + dataset + '/'\n",
    "        # Get list of all image names\n",
    "        image_names = os.listdir(dataset_directory + 'original')\n",
    "        for image_name in image_names:\n",
    "            # image_scales dict for storing filepaths of original and scaled images\n",
    "            # key = scale\n",
    "            # value = filepath of image\n",
    "            image_scales = {}\n",
    "            image_scales['original'] = dataset_directory + 'original/' + image_name\n",
    "            for scale in scales:\n",
    "                image_scales[scale] = dataset_directory + scale + '/' + image_name\n",
    "            # image name points to dictionary of scales\n",
    "            images[image_name] = image_scales\n",
    "    return images\n",
    "\n",
    "num_images = len(testing_dict()) \n",
    "print(\"Number of Images in Testing Dataset: \" + str(num_images))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1zKSliYU9uxk"
   },
   "source": [
    "A machine learning program's data generator feeds information to the model during training. Our [`DataGenerator`](https://github.com/umd-fire-coml/2020-Image-Super-Resolution/blob/master/datagenerator.py) loads LR and HR image pairs from the dictionaries, processes them so that they are compatible with the model, and then outputs batches of data as arrays.\n",
    "\n",
    "We're using saved weights, so it won't be generating batches for training here. `testing_generator` outputs batches containing one LR image and HR image pair from the testing dataset, as this code block demonstrates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KIUop4NF3mih"
   },
   "outputs": [],
   "source": [
    "# Generates batches of LR and HR pairs\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, scale, batch_size, dictionary = \"train\", shuffle=True):\n",
    "        'Initialization'\n",
    "        if dictionary == \"test\":\n",
    "          self.images = testing_dict()\n",
    "        else:\n",
    "          self.images = training_dict()\n",
    "        self.scale = scale\n",
    "        self.r = int(scale[-1])\n",
    "        self.list_IDs = list(self.images.keys())\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Makes one batch of data'\n",
    "        indexes = self.indexes[index*self.batch_size: (index+1)*self.batch_size] \n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes] \n",
    "        # generate data\n",
    "        X = self.__data_generation(list_IDs_temp)\n",
    "        return X\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' \n",
    "        LR = []\n",
    "        HR = []\n",
    "        min_height_LR = maxsize\n",
    "        min_width_LR = maxsize\n",
    "        # Append images as arrays to LR and HR\n",
    "        for ID in list_IDs_temp:\n",
    "            low_res = keras.preprocessing.image.load_img(self.images[ID][self.scale])\n",
    "            high_res = keras.preprocessing.image.load_img(self.images[ID]['original'])\n",
    "            low_res = np.asarray(low_res)\n",
    "            high_res = np.asarray(high_res)\n",
    "            low_res = low_res.astype('float32')\n",
    "            high_res = high_res.astype('float32')\n",
    "            # Normalize images to [0,1]\n",
    "            low_res /= 255.0\n",
    "            high_res /= 255.0\n",
    "            LR.append(low_res)\n",
    "            HR.append(high_res)\n",
    "            # Find the minimum LR dimensions \n",
    "            min_height_LR = min(min_height_LR, low_res.shape[0])\n",
    "            min_width_LR = min(min_width_LR, low_res.shape[1])\n",
    "        # HR/SR image is bigger by a factor of r\n",
    "        min_height_HR = self.r * min_height_LR\n",
    "        min_width_HR = self.r * min_width_LR\n",
    "        for i in range (0, len(LR)):\n",
    "            # Crop LR and HR images to have the same dimensions \n",
    "            LR[i] = self.crop_center(LR[i], min_width_LR, min_height_LR)\n",
    "            HR[i] = self.crop_center(HR[i], min_width_HR, min_height_HR)\n",
    "        LR = np.asarray(LR)\n",
    "        HR = np.asarray(HR)    \n",
    "        return LR, HR \n",
    "    \n",
    "    def crop_center(self, img, min_width, min_height):        \n",
    "        'Crops image around the center given minimum width and height'\n",
    "        width = img.shape[1]\n",
    "        height = img.shape[0]\n",
    "        # Calculates new boundaries around the center\n",
    "        left = int(np.ceil((width - min_width) / 2))\n",
    "        right = left + min_width\n",
    "        top = int(np.ceil((height - min_height) / 2))\n",
    "        bottom = top + min_height\n",
    "        # Crop original image\n",
    "        cropped_img = img[top:bottom, left:right, ...]\n",
    "        return cropped_img\n",
    "\n",
    "testing_generator = DataGenerator('LRbicx' + str(r), batch_size = 1, dictionary = \"test\")\n",
    "\n",
    "# Display a random LR, HR pair\n",
    "lr, hr = testing_generator.__getitem__(random.randint(0,num_images - 10))\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches(10, 10)\n",
    "ax1 = fig.add_subplot(1,2,1)\n",
    "ax1.set_title('Low Resolution (LR): ' + str(lr[0].shape[0]) + ' x ' + str(lr[0].shape[1]) + ' pixels')\n",
    "ax1.imshow(lr[0])\n",
    "ax2 = fig.add_subplot(1,2,2)\n",
    "ax2.set_title('High Resolution (HR): ' + str(hr[0].shape[0]) + ' x ' + str(hr[0].shape[1]) + ' pixels')\n",
    "ax2.imshow(hr[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YNYVXgvv-vkI"
   },
   "source": [
    "## Testing and Performing Single Image Super-Resolution\n",
    "Now, we have what we need to perform SISR. We randomly generate a LR and HR pair from the training dataset using the data generator. Then, we use the trained model to predict a SR image from the LR image. Finally, we display the three images and their dimensions side-by-side to qualitatively compare them to each other. \n",
    "\n",
    "The [Peak Signal to Noise Ratio (PSNR)](https://github.com/umd-fire-coml/2020-Image-Super-Resolution/blob/master/psnr.py) is a quantitative measurement that represents the distance between a prediction and its ground truth; the higher the PSNR, the higher the quality of the prediction. We calculate the PSNR of the SR image and the HR image.\n",
    "\n",
    "**Try running this code block multiple times to see how well the model performs with different images.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_UdzuRpB3fAq"
   },
   "outputs": [],
   "source": [
    "# Generate random LR, HR and predict SR\n",
    "lr, hr = testing_generator.__getitem__(random.randint(0,num_images - 10))\n",
    "sr = model.predict(lr)\n",
    "\n",
    "# Display Images Side by Side\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches(28, 28)\n",
    "ax1 = fig.add_subplot(1,3,1)\n",
    "ax1.set_title('Low Resolution (LR): ' + str(lr[0].shape[0]) + ' x ' + str(lr[0].shape[1]) + ' pixels')\n",
    "ax1.imshow(lr[0])\n",
    "ax2 = fig.add_subplot(1,3,2)\n",
    "ax2.set_title('Super Resolution (SR): ' + str(sr[0].shape[0]) + ' x ' + str(sr[0].shape[1]) + ' pixels')\n",
    "ax2.imshow(sr[0])\n",
    "ax3 = fig.add_subplot(1,3,3)\n",
    "ax3.set_title('High Resolution (HR): ' + str(hr[0].shape[0]) + ' x ' + str(hr[0].shape[1]) + ' pixels')\n",
    "ax3.imshow(hr[0])\n",
    "plt.show()\n",
    "\n",
    "# Peak Signal to Noise Ratio\n",
    "def psnr(oldimg, newimg):\n",
    "    mse = np.mean((oldimg.astype(float) - newimg.astype(float)) ** 2)\n",
    "    if mse != 0:\n",
    "        max_pixel = 1.0\n",
    "        return 20 * math.log10(max_pixel / math.sqrt(mse))\n",
    "    else:\n",
    "        return -1\n",
    "print(\"PSNR(SR, HR): \" + str(psnr(hr[0], sr[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "81iFXuTS_dQP"
   },
   "source": [
    "# Results and Further Improvements\n",
    "As you can see from your results and the example below, the SR image generated by our model is both qualitatively and quantitatively higher quality than the LR image. The PSNR of the SR and HR image is always greater than 10, indicating that they very similar.\n",
    "![example](https://i.imgur.com/ToXzT1w.png)\n",
    "\n",
    "The results are decent, but it is visibly apparent that the SR image is not quite on par with the HR image. Therefore, there is room for improvement:\n",
    "* Create a model with more layers in hopes of capturing more information during training. This is called creating a deeper model.\n",
    "* Add more images to the training dataset to provide more input to the model. This is called creating a wider model.\n",
    "* Train for more epochs (train this model longer). More information can be captured this way, but it is possible to overfit a model and negatively impact your results.  \n",
    "* Fine tune the convolutional layers in `espcn_model` through experimentation to more effectively capture information from the LR images in the training dataset. Since we chose the parameters for `Conv2D` as estimates, this is our most promising next step. \n",
    "\n",
    "Improvements to the program would lead to even clearer SR images with higher PSNR scores.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sm1KJvEhNqgo"
   },
   "source": [
    "# Try it Yourself\n",
    "This model doesn't just work with images generated by our generator; you can use it on your own images as well! Change `url` to the image address of your image, then run the code to super-resolution it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rtCRle9sNplD"
   },
   "outputs": [],
   "source": [
    "url = \"https://i.imgur.com/oIm04AT.png\"\n",
    "response = requests.get(url)\n",
    "image = imageio.imread(url)\n",
    "\n",
    "\n",
    "# Load image and convert it to compatible LR format\n",
    "# image = keras.preprocessing.image.load_img('/content/drive/Shared drives/COML-STUDENTS-2020/Fall/Team Projects/T4 Image Super-Resolution/' + image_name)\n",
    "image = np.asarray(image)\n",
    "image = image.astype('float32')\n",
    "image /= 255.0\n",
    "LR = [image]\n",
    "LR = np.asarray(LR)\n",
    "# Predict SR image\n",
    "SR = model.predict(LR)\n",
    "# Display images side by side\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches(20, 20)\n",
    "ax1 = fig.add_subplot(1,2,1)\n",
    "ax1.set_title('Low Resolution (LR): ' + str(lr[0].shape[0]) + ' x ' + str(lr[0].shape[1]) + ' pixels')\n",
    "ax1.imshow(LR[0])\n",
    "ax2 = fig.add_subplot(1,2,2)\n",
    "ax2.set_title('Super Resolution (SR): ' + str(hr[0].shape[0]) + ' x ' + str(hr[0].shape[1]) + ' pixels')\n",
    "ax2.imshow(SR[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Image Super Resolution - FIRE Summit.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
